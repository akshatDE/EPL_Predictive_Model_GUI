{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Saved page 1 to 'data/page_1.html'\n",
      "Scraping page 2...\n",
      "Saved page 2 to 'data/page_2.html'\n",
      "Scraping page 3...\n",
      "Saved page 3 to 'data/page_3.html'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Function to scrape HTML files from Amazon\n",
    "def scrape_amazon_pages(query=\"laptops\", pages=3):\n",
    "    # Configure user-agent and proxies for stealth\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "    chrome_options.add_argument(\"--headless\")  # Uncomment this for headless mode\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Create a directory to save HTML files\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        try:\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            url = f\"https://www.amazon.com/s?k={query.replace(' ', '+')}&page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for product containers to load\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"//div[@data-component-type='s-search-result']\"))\n",
    "            )\n",
    "\n",
    "            # Scroll to the bottom of the page to load all dynamic content\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.randint(5, 10))  # Random delay to mimic human interaction\n",
    "\n",
    "            # Save the HTML content to a file\n",
    "            with open(f\"data/page_{page}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(driver.page_source)\n",
    "            print(f\"Saved page {page} to 'data/page_{page}.html'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "            if \"sorry\" in driver.page_source.lower():  # Check for block\n",
    "                print(\"Blocked by Amazon. Adding delay...\")\n",
    "                time.sleep(60)  # Wait 1 minute before retrying\n",
    "        finally:\n",
    "            time.sleep(random.randint(5, 15))  # Random delay between requests\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"Scraping completed!\")\n",
    "\n",
    "\n",
    "# Function to parse HTML files and extract data\n",
    "def parse_saved_pages():\n",
    "    data = {\n",
    "        \"Title\": [],\n",
    "        \"Price\": [],\n",
    "        \"Rating\": [],\n",
    "        \"Number_of_Reviews\": [],\n",
    "        \"Availability\": [],\n",
    "        \"Brand\": [],\n",
    "        \"Discount_Price\": [],\n",
    "        \"Link\": [],\n",
    "        \"RAM_Storage\": [],\n",
    "        \"Color\": []\n",
    "    }\n",
    "\n",
    "    for file in os.listdir(\"data\"):\n",
    "        try:\n",
    "            # Open and parse the HTML file\n",
    "            with open(f\"data/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "                html_doc = f.read()\n",
    "            soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "\n",
    "            # Extract product containers\n",
    "            items = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "            for item in items:\n",
    "                # Extract Title\n",
    "                try:\n",
    "                    title = item.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\").get_text(strip=True)\n",
    "                except:\n",
    "                    title = \"N/A\"\n",
    "\n",
    "                # Extract Price\n",
    "                try:\n",
    "                    price_whole = item.find(\"span\", class_=\"a-price-whole\")\n",
    "                    price_fraction = item.find(\"span\", class_=\"a-price-fraction\")\n",
    "                    price = float(price_whole.get_text(strip=True) + \".\" + price_fraction.get_text(strip=True)) if price_whole and price_fraction else None\n",
    "                except:\n",
    "                    price = None\n",
    "\n",
    "                # Extract Rating\n",
    "                try:\n",
    "                    rating = item.find(\"span\", class_=\"a-icon-alt\").get_text(strip=True).split()[0]\n",
    "                except:\n",
    "                    rating = None\n",
    "\n",
    "                # Extract Number of Reviews\n",
    "                try:\n",
    "                    reviews = item.find(\"span\", class_=\"a-size-base\").get_text(strip=True).replace(\",\", \"\")\n",
    "                    number_of_reviews = int(reviews) if reviews.isdigit() else None\n",
    "                except:\n",
    "                    number_of_reviews = None\n",
    "\n",
    "                # Extract Availability\n",
    "                try:\n",
    "                    availability = \"In Stock\" if item.find(\"span\", class_=\"a-color-success\") else \"Out of Stock\"\n",
    "                except:\n",
    "                    availability = \"Unknown\"\n",
    "\n",
    "                # Extract Brand\n",
    "                try:\n",
    "                    brand = title.split()[0] if title != \"N/A\" else \"Unknown\"\n",
    "                except:\n",
    "                    brand = \"Unknown\"\n",
    "\n",
    "                # Calculate Discount Price (10% Discount)\n",
    "                try:\n",
    "                    discount_price = round(price * 0.9, 2) if price else None\n",
    "                except:\n",
    "                    discount_price = None\n",
    "\n",
    "                # Extract Product Link\n",
    "                try:\n",
    "                    link_tag = item.find(\"a\", class_=\"a-link-normal\", href=True)\n",
    "                    link = f\"https://www.amazon.com{link_tag['href']}\" if link_tag else \"N/A\"\n",
    "                except:\n",
    "                    link = \"N/A\"\n",
    "\n",
    "                # Extract RAM and Storage (if available)\n",
    "                try:\n",
    "                    ram_storage = item.find(\"span\", class_=\"selection\").get_text(strip=True)\n",
    "                except:\n",
    "                    ram_storage = \"N/A\"\n",
    "\n",
    "                # Extract Color (if available)\n",
    "                try:\n",
    "                    color = item.find(\"span\", class_=\"selection\").get_text(strip=True)\n",
    "                except:\n",
    "                    color = \"N/A\"\n",
    "\n",
    "                # Append data\n",
    "                data[\"Title\"].append(title)\n",
    "                data[\"Price\"].append(price)\n",
    "                data[\"Rating\"].append(rating)\n",
    "                data[\"Number_of_Reviews\"].append(number_of_reviews)\n",
    "                data[\"Availability\"].append(availability)\n",
    "                data[\"Brand\"].append(brand)\n",
    "                data[\"Discount_Price\"].append(discount_price)\n",
    "                data[\"Link\"].append(link)\n",
    "                data[\"RAM_Storage\"].append(ram_storage)\n",
    "                data[\"Color\"].append(color)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    # Save the extracted data to a CSV file\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"parsed_amazon_laptops.csv\", index=False)\n",
    "    print(\"Parsed data saved to 'parsed_amazon_laptops.csv'\")\n",
    "\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Scrape Amazon pages\n",
    "    scrape_amazon_pages(query=\"laptops\", pages=3)\n",
    "\n",
    "    # Step 2: Parse saved HTML files\n",
    "    parse_saved_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_1.html', 'r') as file:\n",
    "    data = file.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
